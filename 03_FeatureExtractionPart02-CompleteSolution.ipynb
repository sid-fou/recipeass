{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e71f2dc",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a40ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e6d1f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16e5a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-8G9M266:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Basics</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x26869ace9d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccbec7ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'install_pypi_package'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_168\\2302286864.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run this everytime you create a new spark instance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall_pypi_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"plotly==5.5.0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall_pypi_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pandas==0.25.1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall_pypi_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"numpy==1.14.5\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'install_pypi_package'"
     ]
    }
   ],
   "source": [
    "# Run this everytime you create a new spark instance. \n",
    "\n",
    "spark.sparkContext.install_pypi_package(\"plotly==5.5.0\")\n",
    "spark.sparkContext.install_pypi_package(\"pandas==0.25.1\")\n",
    "spark.sparkContext.install_pypi_package(\"numpy==1.14.5\")\n",
    "spark.sparkContext.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea1e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Import for typecasting columns\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
    "from pyspark.sql.types import ArrayType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b75577",
   "metadata": {},
   "source": [
    "## Defining Custom Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "515ff4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quantiles(df, col_name, quantiles_list = [0.01, 0.25, 0.5, 0.75, 0.99]):\n",
    "    \"\"\"\n",
    "    Takes a numerical column and returns column values at requested quantiles\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Dataframe\n",
    "    Argument 2: Name of the column\n",
    "    Argument 3: A list of quantiles you want to find. Default value [0.01, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "    Output \n",
    "    Returns a dictionary with quantiles as keys and column quantile values as values \n",
    "    \"\"\"\n",
    "    # Get min, max and quantile values for given column\n",
    "    min_val = df.agg(F.min(col_name)).first()[0]\n",
    "    max_val = df.agg(F.max(col_name)).first()[0]\n",
    "    quantiles_vals = df.approxQuantile(col_name,\n",
    "                                       quantiles_list,\n",
    "                                       0)\n",
    "  \n",
    "    # Store min, quantiles and max in output dict, sequentially\n",
    "    quantiles_dict = {0.0:min_val}\n",
    "    quantiles_dict.update(dict(zip(quantiles_list, quantiles_vals)))\n",
    "    quantiles_dict.update({1.0:max_val})\n",
    "    return(quantiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca215059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bucketwise_statistics (summary, bucketizer):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe and a bucketizer object and plots the summary statistics for each bucket in the dataframe. \n",
    "  \n",
    "    Inputs\n",
    "    Argument 1: Pandas dataframe obtained from bucket_col_print_summary function \n",
    "    Argument 2: Bucketizer object obtained from bucket_col_print_summary function\n",
    "  \n",
    "    Output\n",
    "    Displays a plot of bucketwise average ratings nunber of ratings of a parameter.   \n",
    "    \"\"\"\n",
    "    # Creating bucket labels from splits\n",
    "    classlist = bucketizer.getSplits()\n",
    "    number_of_classes = len(classlist) - 1\n",
    "\n",
    "    class_labels = []\n",
    "    hover_labels = []\n",
    "    for i in range (number_of_classes):\n",
    "        hover_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) +\" (Bucket name: \"+ str(int(i)) +\")\"  )\n",
    "        class_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) )\n",
    "  \n",
    "    summary[\"Scaled_number\"] = (summary[\"n_ratings\"]-summary[\"n_ratings\"].min())/(summary[\"n_ratings\"].max()-summary[\"n_ratings\"].min()) + 1.5\n",
    "    summary['Bucket_Names'] = class_labels\n",
    "  \n",
    "    # making plot\n",
    "    x = summary[\"Bucket_Names\"]\n",
    "    y1 = summary[\"avg_rating\"]\n",
    "    y2 = summary[\"n_ratings\"]\n",
    "    err = summary[\"stddev_rating\"]  \n",
    "\n",
    "    # Plot scatter here\n",
    "    plt.rcParams[\"figure.figsize\"] = [summary.shape[0]+2, 6.0]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    bar = ax1.bar(x, y1, color = \"#262261\")\n",
    "    ax1.errorbar(x, y1, yerr=err, fmt=\"o\", color=\"#EE4036\")\n",
    "    ax1.set(ylim=(0, 7))\n",
    "  \n",
    "    #ax1.bar_label(bar , fmt='%.2f', label_type='edge')  \n",
    "    def barlabel(x_list,y_list):\n",
    "        for i in range(len(x_list)):\n",
    "            ax1.text(i,y_list[i] + 0.2,y_list[i], ha = 'center',\n",
    "  \t\t\t         fontdict=dict(size=10),\n",
    "  \t\t\t         bbox=dict(facecolor='#262261', alpha=0.2)         \n",
    "  \t\t\t        )\n",
    "    barlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"avg_rating\"].round(2).tolist())\n",
    "  \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x, y2, s=summary[\"Scaled_number\"]*500, c = '#FAAF40')  \n",
    "    ax2.set(ylim=(0, summary[\"n_ratings\"].max()*1.15))\n",
    "    def scatterlabel(x_list,y_list):\n",
    "  \t    for i in range(len(x_list)):\n",
    "  \t\t    ax2.text(i,y_list[i] + 15000,y_list[i], ha = 'center',\n",
    "  \t\t\t\t\t fontdict=dict(size=10),\n",
    "                     bbox=dict(facecolor='#FAAF40', alpha=0.5)\n",
    "  \t\t\t\t\t)\n",
    "    scatterlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"n_ratings\"].tolist())\n",
    "  \n",
    "    # giving labels to the axises\n",
    "    ax1.set_xlabel(bucketizer.getOutputCol(), fontdict=dict(size=14)) \n",
    "    ax1.set_ylabel(\"Average Ratings\",fontdict=dict(size=14))\n",
    "  \n",
    "    # secondary y-axis label\n",
    "    ax2.set_ylabel('Number of Ratings',fontdict=dict(size=14))\n",
    "  \n",
    "    #plot Title\n",
    "    plt.title('Bucketwise average ratings and number of ratings for \\n'+bucketizer.getInputCol(), \n",
    "              fontdict=dict(size=14)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2da73721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucket_col_print_summary(df, splits, inputCol, outputCol):\n",
    "    \"\"\"\n",
    "    Given a numerical column in a data frame, adds a bucketized version of the column to the data frame, according to splits provided.\n",
    "    Also prints a summary of ratings seen in each bucket made.\n",
    "\n",
    "    Inputs \n",
    "    Argument 1: Data Frame \n",
    "    Argument 2: Values at which the column will be split\n",
    "    Argument 3: Name of the input column (numerical column)\n",
    "    Argument 4: Name of the output column (bucketized numerical column)\n",
    "\n",
    "    Output: \n",
    "    1) New dataframe with the output column added\n",
    "    2) Bucketizer object trained from the input column \n",
    "    3) Pandas dataframe with summary statistics for ratings seen in buckets of the output column\n",
    "    Also plots summary statistics for ratings seen in buckets of the output column\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropping bucket if it already exists\n",
    "    if outputCol in df.columns:\n",
    "        df = df.drop(outputCol)\n",
    "\n",
    "    # Training bucketizer\n",
    "    bucketizer = Bucketizer(splits = splits,\n",
    "                            inputCol  = inputCol,\n",
    "                            outputCol = outputCol)\n",
    "    \n",
    "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
    "\n",
    "    # Printing meta information on buckets created\n",
    "    print(\"Added bucketized column {}\".format(outputCol))\n",
    "    print(\"\")\n",
    "    print(\"Bucketing done for split definition: {}\".format(splits))\n",
    "    print(\"\")  \n",
    "    print(\"Printing summary statistics for ratings in buckets below:\")\n",
    "\n",
    "    # Creating a summary statistics dataframe and passing it to the plotting function\n",
    "    summary =  (df\n",
    "                .groupBy(outputCol)\n",
    "                .agg(F.avg('rating').alias('avg_rating'),\n",
    "                     F.stddev('rating').alias('stddev_rating'),\n",
    "                     F.count('rating').alias('n_ratings'))\n",
    "                .sort(outputCol)\n",
    "                .toPandas())\n",
    "  \n",
    "    plot_bucketwise_statistics(summary,bucketizer)\n",
    "  \n",
    "    return df, bucketizer, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71bd221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_distribution_summary(df, col_name):\n",
    "    \"\"\"\n",
    "    Takes a column in a data frame and prints the summary statistics (average, standard deviation, count and distinct count) for all unique values in that column.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Name of the column\n",
    "  \n",
    "    Output\n",
    "    Returns nothing \n",
    "    Prints a Dataframe with summary statistics\n",
    "    \"\"\"\n",
    "    print(df\n",
    "          .groupBy(col_name)\n",
    "          .agg(F.avg('rating').alias('avg_rating'),\n",
    "               F.stddev('rating').alias('stddev_rating'),\n",
    "               F.count('rating').alias('n_ratings'),\n",
    "               F.countDistinct('id').alias('n_recipes'))\n",
    "          .sort(F.col(col_name).asc())\n",
    "          .show(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79011c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_items_satisfying_condition (df, condition, aggregation_level = \"recipe\"):\n",
    "    \"\"\"\n",
    "    Given a condition, find the number of recipes / reviews that match the condition.\n",
    "    Also calculates the percentage of such recipes / reviews as a percentage of all recipes / reviews.\n",
    "  \n",
    "    Inputs \n",
    "    Argument 1: Dataframe \n",
    "    Argument 2: Logical expression describing a condition, string type. eg: \"minutes == 0\"\n",
    "    Argument 3: Aggregation level for determining \"items\", either  \"recipe\" or \"review\". Default value == \"recipe\"\n",
    "  \n",
    "    Output: Returns no object.\n",
    "    Prints the following:\n",
    "    1) Number of recipes / reviews that satisfy the condition\n",
    "    2) Total number of recipes / reviews in the dataframe\n",
    "    3) Percentage of recipes / reviews that satisfy the condition\n",
    "    \"\"\"\n",
    "    # Find out num rows satisfying the condition\n",
    "    if aggregation_level == \"recipe\": \n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\"))).first()[0]\n",
    "      \n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\"))).first()[0]\n",
    "    if aggregation_level == \"review\":\n",
    "        number_of_rows_satisfying_condition = (df\n",
    "                                             .filter(condition)\n",
    "                                             .agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "        n_rows_total = (df.agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
    "  \n",
    "    # Find out % rows satisfying the conditon and print a properly formatted output\n",
    "    perc_rows = round(number_of_rows_satisfying_condition * 100/ n_rows_total, 2)\n",
    "    print('Condition String                   : \"{}\"'.format(condition))\n",
    "    print(\"Num {}s Satisfying Condition   : {} [{}%]\".format(aggregation_level.title(), number_of_rows_satisfying_condition, perc_rows))\n",
    "    print(\"Total Num {}s                  : {}\".format(aggregation_level.title(), n_rows_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ec89e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_OHE_columns (df, n_name_list):\n",
    "    \"\"\"\n",
    "    Given a list of tags, creates one hot encoded columns for each tag. \n",
    "  \n",
    "    Input\n",
    "    Argument 1: Dataframe in which the function will add the new columns\n",
    "    Argument 2: list of tags\n",
    "  \n",
    "    Output\n",
    "    Prints the names of columns that have been added \n",
    "    Returns the modified dataframe \n",
    "    \"\"\"\n",
    "    for name in n_name_list:\n",
    "        df = (df.withColumn(\"has_tag_\"+name, F.when(F.array_contains(df.tags, name), 1).otherwise(0)))\n",
    "        print (\"added column: has_tag_\"+name)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f974fc0",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6716e7a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_168\\3110069737.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minteraction_level_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./interaction_level'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[1;34m(self, *paths, **options)\u001b[0m\n\u001b[0;32m    362\u001b[0m         )\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     def text(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r\n\tat org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r\n\tat org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r\n\tat org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\r\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n"
     ]
    }
   ],
   "source": [
    "interaction_level_df = spark.read.parquet('./interaction_level')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba87c0",
   "metadata": {},
   "source": [
    "## Adding user level average features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e70465",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = Window.partitionBy(\"user_id\")\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"user_avg_rating\",\n",
    "                                    F.avg(F.col(\"rating\")).over(partition))\n",
    "                        .withColumn(\"user_n_ratings\",\n",
    "                                    F.count(F.col(\"rating\")).over(partition))\n",
    "                        .withColumn(\"user_avg_years_betwn_review_and_submission\",\n",
    "                                    F.avg(F.col(\"years_since_submission_on_review_date\")).over(partition))\n",
    "                        .withColumn(\"user_avg_prep_time_recipes_reviewed\",\n",
    "                                    F.avg(F.col(\"minutes\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_steps_recipes_reviewed\",\n",
    "                                    F.avg(F.col(\"n_steps\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_ingredients_recipes_reviewed\",\n",
    "                                    F.avg(F.col(\"n_ingredients\")).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab88a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nutrition_cols = ['calories',\n",
    "                  'total_fat_per_100_cal',\n",
    "                  'sugar_per_100_cal',\n",
    "                  'sodium_per_100_cal',\n",
    "                  'protein_per_100_cal',\n",
    "                  'saturated_fat_per_100_cal',\n",
    "                  'carbohydrates_per_100_cal']\n",
    "\n",
    "for nutri_col in nutrition_cols:\n",
    "    interaction_level_df = (interaction_level_df\n",
    "                            .withColumn(\"user_avg_{}_recipes_reviewed\".format(nutri_col),\n",
    "                                        F.avg(F.col(nutri_col)).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12589059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code check cell\n",
    "# Do not edit cells with assert commands\n",
    "# If an error is shown after running this cell, please recheck your code. \n",
    "\n",
    "assert(round(interaction_level_df.filter('user_id == 601529').select('user_avg_rating').first()[0], 2) == 4.22)\n",
    "assert(interaction_level_df.filter('user_id == 601529').select('user_n_ratings').first()[0] == 27)\n",
    "assert(round(interaction_level_df.filter('user_id == 601529').select('user_avg_years_betwn_review_and_submission').first()[0], 2) == 3.51)\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_prep_time_recipes_reviewed').first()[0] == 50.3)\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_n_steps_recipes_reviewed').first()[0] == 8.8)\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_n_ingredients_recipes_reviewed').first()[0] == 8.2)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_total_fat_per_100_cal_recipes_reviewed').first()[0]) == 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513b134",
   "metadata": {},
   "source": [
    "**More Features:**\n",
    "\n",
    "high_ratings = 5 rating\n",
    "\n",
    "- `user_avg_years_betwn_review_and_submission_high_ratings`\n",
    "- `user_avg_prep_time_recipes_reviewed_high_ratings`\n",
    "- `user_avg_n_steps_recipes_reviewed_high_ratings`\n",
    "- `user_avg_n_ingredients_recipes_reviewed_high_ratings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa15a2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"ind_5_rating\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(1))\n",
    "                        .withColumn(\"years_since_submission_on_review_date_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"years_since_submission_on_review_date\")))\n",
    "                        .withColumn(\"minutes_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"minutes\")))\n",
    "                        .withColumn(\"n_steps_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"n_steps\")))\n",
    "                        .withColumn(\"n_ingredients_5_ratings\",\n",
    "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                     .otherwise(F.col(\"n_ingredients\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = Window.partitionBy(\"user_id\")\n",
    "\n",
    "interaction_level_df = (interaction_level_df\n",
    "                        .withColumn(\"user_n_5_ratings\",\n",
    "                                    F.sum(F.col(\"ind_5_rating\")).over(partition))\n",
    "                        .withColumn(\"user_avg_years_betwn_review_and_submission_5_ratings\",\n",
    "                                    F.avg(F.col(\"years_since_submission_on_review_date_5_ratings\")).over(partition))\n",
    "                        .withColumn(\"user_avg_prep_time_recipes_reviewed_5_ratings\",\n",
    "                                    F.avg(F.col(\"minutes_5_ratings\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_steps_recipes_reviewed_5_ratings\",\n",
    "                                    F.avg(F.col(\"n_steps_5_ratings\")).over(partition))\n",
    "                        .withColumn(\"user_avg_n_ingredients_recipes_reviewed_5_ratings\",\n",
    "                                    F.avg(F.col(\"n_ingredients_5_ratings\")).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nutri_col in nutrition_cols:\n",
    "    interaction_level_df = (interaction_level_df\n",
    "                            .withColumn(\"{}_5_ratings\".format(nutri_col),\n",
    "                                        F.when(interaction_level_df[\"rating\"] != 5, None)\n",
    "                                         .otherwise(F.col(nutri_col))))\n",
    "    interaction_level_df = (interaction_level_df\n",
    "                            .withColumn(\"user_avg_{}_recipes_reviewed_5_ratings\".format(nutri_col),\n",
    "                                        F.avg(F.col(\"{}_5_ratings\".format(nutri_col))).over(partition)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check - All rows with ratings should have non-null values in corresponding user_avg_5_ratings columns\n",
    "\n",
    "assert(interaction_level_df\n",
    "       .filter(\"rating == 5\")\n",
    "       .filter(interaction_level_df.user_n_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_years_betwn_review_and_submission_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_prep_time_recipes_reviewed_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_n_steps_recipes_reviewed_5_ratings.isNull() |\n",
    "               interaction_level_df.user_avg_n_ingredients_recipes_reviewed_5_ratings.isNull())\n",
    "       .count() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcdbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values for a given user id\n",
    "\n",
    "assert(interaction_level_df.filter('user_id == 233044').select('user_n_5_ratings').first()[0] == 7)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_years_betwn_review_and_submission_5_ratings').first()[0], 2) == 2.24)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_prep_time_recipes_reviewed_5_ratings').first()[0]) == 46)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_n_steps_recipes_reviewed_5_ratings').first()[0], 2) == 7.29)\n",
    "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_n_ingredients_recipes_reviewed_5_ratings').first()[0], 2) == 6.86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b122a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947f240",
   "metadata": {},
   "source": [
    "## Tags level EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0537107",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_tag_level_df = interaction_level_df.withColumn('individual_tag',F.explode('tags'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary = (interaction_tag_level_df\n",
    "                        .groupBy('individual_tag').agg(F.avg('rating').alias('avg_user_rating'),\n",
    "#                                                      F.max('rating').alias('max_user_rating'),\n",
    "#                                                      F.min('rating').alias('min_user_rating'),\n",
    "                                                       F.count('rating').alias('n_user_ratings'),\n",
    "                                                       F.countDistinct('id').alias('n_recipes')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea4dfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interactions, recipes  =  interaction_level_df.count(), interaction_level_df.agg(F.countDistinct('id')).first()[0]\n",
    "\n",
    "tags_ratings_summary = (tags_ratings_summary.withColumn(\"in_percent_recipies\", F.col (\"n_recipes\")/F.lit(recipes))\n",
    "                                            .withColumn(\"in_percent_interactions\", F.col (\"n_user_ratings\")/F.lit(interactions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b73f1b2",
   "metadata": {},
   "source": [
    "#### 1. Top ```n``` most rated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8969bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary.sort(F.col(\"n_user_ratings\").desc()).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3b60e",
   "metadata": {},
   "source": [
    "Drop tags present in majority of recipes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8601fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary = tags_ratings_summary.filter(tags_ratings_summary.in_percent_interactions < 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_most_frequent_tags = tags_ratings_summary.sort(F.col(\"n_user_ratings\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = top_most_frequent_tags , \n",
    "              col_name = 'in_percent_interactions', \n",
    "              quantiles_list = [0.01,0.25,0.5, 0.75,0.8,0.85,0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc86bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tags appearing in the top 5 percentile \n",
    "top_most_frequent_tags = top_most_frequent_tags.filter(\"in_percent_interactions > 0.16\")\n",
    "\n",
    "top_most_frequent_tags.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_frequent_tags_list = [data[0] for data in top_most_frequent_tags.select('individual_tag').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = add_OHE_columns (interaction_level_df, top_frequent_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33744553",
   "metadata": {},
   "source": [
    "#### 2.  Bottom ```n``` least rated tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary.sort(F.col(\"n_user_ratings\").asc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28edd839",
   "metadata": {},
   "source": [
    "The above tags are present in 1 recipe in over two hundred thousand. The features we create based on these tags will not teach the model new information. If these tags were one hot encoded, the entire column would be filled with zeros, and only a few rows will have 1s. One hot encoding of these tags is not a good idea. If you come up with an encoding that captures the rarity of these tags, only then can you add these tags to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bacac2",
   "metadata": {},
   "source": [
    "#### 3. Top ```n``` rated tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4391c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary.sort(F.col(\"avg_user_rating\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba743fa",
   "metadata": {},
   "source": [
    "Top rated tags have low number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c62424",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles (tags_ratings_summary, \"n_user_ratings\", quantiles_list = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ratings_summary = tags_ratings_summary.filter(tags_ratings_summary.n_user_ratings > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52df5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rated_tags_df = tags_ratings_summary.sort(F.col(\"avg_user_rating\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d9eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles(df = top_rated_tags_df , \n",
    "              col_name = 'avg_user_rating', \n",
    "              quantiles_list = [0.01,0.25,0.5, 0.75,0.8,0.85,0.9,0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep tags above 95 percentile\n",
    "top_rated_tags_df = top_rated_tags_df.filter(\"avg_user_rating > 4.53\")\n",
    "\n",
    "top_rated_tags_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ccccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rated_tags_list = [data[0] for data in top_rated_tags_df.select('individual_tag').collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe9edf",
   "metadata": {},
   "source": [
    "Check if any of the current tags have been added earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaafc25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(top_frequent_tags_list) & set(top_rated_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_added_columns_set = set(top_frequent_tags_list).union(set(top_rated_tags_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5693caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df = add_OHE_columns (interaction_level_df, top_rated_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06488a",
   "metadata": {},
   "source": [
    "#### 3. Bottom ```n``` rated tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f3abff",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_rated_tags_df = tags_ratings_summary.sort(F.col(\"avg_user_rating\").asc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ffd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_quantiles (bottom_rated_tags_df, \"avg_user_rating\", quantiles_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_rated_tags_df = bottom_rated_tags_df.filter(\"avg_user_rating < 4.00\")\n",
    "\n",
    "bottom_rated_tags_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_rated_tags_list = [data[0] for data in bottom_rated_tags_df.select('individual_tag').collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_added_columns_set & set(bottom_rated_tags_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df =  add_OHE_columns(interaction_level_df, bottom_rated_tags_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedadf63",
   "metadata": {},
   "source": [
    "## Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dffa017",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interaction_level_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_level_df.write.mode('overwrite').parquet('./interaction_levelii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463ff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
